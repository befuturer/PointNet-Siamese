{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import numpy.random as rng\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from models.pointnet2_utils import PointNetSetAbstractionMsg, PointNetSetAbstraction\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class get_model(nn.Module):\n",
    "    def __init__(self,num_class=10,normal_channel=False):\n",
    "        super(get_model, self).__init__()\n",
    "        in_channel = 3 if normal_channel else 0\n",
    "        self.normal_channel = normal_channel\n",
    "        self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]])\n",
    "        self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]])\n",
    "        self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256,5)\n",
    "        self.fc5 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward_once(self, xyz):\n",
    "        B, _, _ = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            norm = xyz[:, 3:, :]\n",
    "            xyz = xyz[:, :3, :]\n",
    "        else:\n",
    "            norm = None\n",
    "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        x = l3_points.view(B, 1024)\n",
    "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "  \n",
    "        output1 = self.forward_once(input1)\n",
    "\n",
    "        output2 = self.forward_once(input2)\n",
    "        \n",
    "#         dis = F.pairwise_distance(output1, output2)\n",
    "#         dis = torch.abs(output1 - output2)\n",
    "\n",
    "#         pre = torch.sigmoid(self.fc5(dis.unsqueeze(-1)))\n",
    "#         pre = self.fc5(dis)\n",
    "#         pre = torch.sigmoid(self.fc5(dis))\n",
    "#         print(\"pre = \", pre)\n",
    "        \n",
    "        return output1,output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "        print(euclidean_distance)\n",
    "\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "#         print(\"test:\", loss_contrastive)\n",
    "        return loss_contrastive*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_normalize(pc):\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(idx = 2120):\n",
    "    npoints = 1024    \n",
    "    split='test'\n",
    "    root = \"/home/jovyan/code/PointCloudClassifier/PointNet/pointnet/modelnet40_normal_resampled\"\n",
    "    num_category = 10 \n",
    "\n",
    "    catfile = os.path.join(root, 'modelnet10_shape_names.txt')\n",
    "\n",
    "    cat = [line.rstrip() for line in open(catfile)]  # 以列表的形式存放40类物品\n",
    "    classes = dict(zip(cat, range(len(cat))))\n",
    "    shape_ids = {}\n",
    "\n",
    "    shape_ids['train'] = [line.rstrip() for line in open(os.path.join(root, 'modelnet10_train.txt'))]\n",
    "    shape_ids['test'] = [line.rstrip() for line in open(os.path.join(root, 'modelnet10_test.txt'))]\n",
    "\n",
    "    assert (split == 'train' or split == 'test')\n",
    "    shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
    "    datapath = [(shape_names[i], os.path.join(root, shape_names[i], shape_ids[split][i]) + '.txt') \n",
    "                for i in range(len(shape_ids[split]))]\n",
    "    \n",
    "#     print('The size of %s data is %d' % (split, len(datapath)))\n",
    "\n",
    "#     np.random.shuffle(datapath)\n",
    "    save_path = os.path.join(root, 'modelnet%d_%s_%dpts.dat' % (num_category, split, npoints))\n",
    "\n",
    "    if split == 'train':    \n",
    "        y_train_point = []\n",
    "        for index in range(len(datapath)):\n",
    "            cls = classes[datapath[index][0]]\n",
    "            y_train_point.append(cls)\n",
    "\n",
    "        y_train_point = np.array(y_train_point).astype(np.int32)\n",
    "\n",
    "        indices = [np.where(y_train_point == i)[0] for i in sorted(list(set(y_train_point)))]\n",
    "        n_classes = len(sorted(list(set(y_train_point))))\n",
    "    if split == 'test':\n",
    "        y_test_point = []\n",
    "        for index in range(len(datapath)):\n",
    "            cls = classes[datapath[index][0]]\n",
    "            y_test_point.append(cls)\n",
    "        \n",
    "        #需要输出，表示分类标签，908\n",
    "        y_test_point = np.array(y_test_point).astype(np.int32)\n",
    "\n",
    "        indices = [np.where(y_test_point == i)[0] for i in sorted(list(set(y_test_point)))]\n",
    "        sort_classes = sorted(list(set(y_test_point)))\n",
    "        n_classes = len(sorted(list(set(y_test_point))))\n",
    "        N = n_classes\n",
    "        \n",
    "  \n",
    "        #这里\n",
    "#         point_set = np.loadtxt(datapath[1][1], delimiter=',').astype(np.float32)[0:npoints, :][:, 0:3]\n",
    "        point_set = pc_normalize(np.loadtxt(datapath[1][1], delimiter=',').astype(np.float32)[0:npoints, :][:, 0:3])#要改为3个长度的数据\n",
    "        w, h = point_set.shape\n",
    "\n",
    "        temp = np.loadtxt(datapath[idx][1], delimiter=',').astype(np.float32)[0:npoints, :][:, 0:3]\n",
    "        test_image = np.asarray([temp]*N).reshape(N, w, h)\n",
    "        \n",
    "        support_set = np.zeros((N,w,h))\n",
    "\n",
    "        for index in range(N):\n",
    "            #这里\n",
    "            support_set[index,:,:] = pc_normalize(np.loadtxt(datapath[int(rng.choice(indices[index],size=(1,),replace=False))][1], delimiter=',').astype(np.float32)[0:npoints, :][:, 0:3])\n",
    "#             support_set[index,:,:] = np.loadtxt(datapath[int(rng.choice(indices[index],size=(1,),replace=False))][1], delimiter=',').astype(np.float32)[0:npoints, :][:, 0:3]\n",
    "            \n",
    "\n",
    "        targets = np.zeros((N,))\n",
    "        \n",
    "        true_index = sort_classes.index(y_test_point[idx])\n",
    "        targets[true_index] = 1\n",
    "\n",
    "        categories = sort_classes\n",
    "        pairs = [test_image,support_set]\n",
    "        pairs = torch.from_numpy(np.array(pairs).astype(np.float32))\n",
    "        pairs = pairs.transpose(3, 2).cuda()\n",
    "        \n",
    "    return pairs,targets,categories,y_test_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_test_point(idx = 2120):\n",
    "    npoints = 1024    \n",
    "    split='test'\n",
    "    root = \"/home/jovyan/code/PointCloudClassifier/PointNet/pointnet/modelnet40_normal_resampled\"\n",
    "    num_category = 10 \n",
    "\n",
    "    catfile = os.path.join(root, 'modelnet10_shape_names.txt')\n",
    "\n",
    "    cat = [line.rstrip() for line in open(catfile)]  # 以列表的形式存放40类物品\n",
    "    classes = dict(zip(cat, range(len(cat))))\n",
    "    shape_ids = {}\n",
    "\n",
    "    shape_ids['train'] = [line.rstrip() for line in open(os.path.join(root, 'modelnet10_train.txt'))]\n",
    "    shape_ids['test'] = [line.rstrip() for line in open(os.path.join(root, 'modelnet10_test.txt'))]\n",
    "\n",
    "    assert (split == 'train' or split == 'test')\n",
    "    shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
    "    datapath = [(shape_names[i], os.path.join(root, shape_names[i], shape_ids[split][i]) + '.txt') \n",
    "                for i in range(len(shape_ids[split]))]\n",
    "    \n",
    "#     print('The size of %s data is %d' % (split, len(datapath)))\n",
    "\n",
    "#     np.random.shuffle(datapath)\n",
    "    save_path = os.path.join(root, 'modelnet%d_%s_%dpts.dat' % (num_category, split, npoints))\n",
    "\n",
    "    if split == 'train':    \n",
    "        y_train_point = []\n",
    "        for index in range(len(datapath)):\n",
    "            cls = classes[datapath[index][0]]\n",
    "            y_train_point.append(cls)\n",
    "\n",
    "        y_train_point = np.array(y_train_point).astype(np.int32)\n",
    "\n",
    "        indices = [np.where(y_train_point == i)[0] for i in sorted(list(set(y_train_point)))]\n",
    "        n_classes = len(sorted(list(set(y_train_point))))\n",
    "    if split == 'test':\n",
    "        y_test_point = []\n",
    "        for index in range(len(datapath)):\n",
    "            cls = classes[datapath[index][0]]\n",
    "            y_test_point.append(cls)\n",
    "        \n",
    "        #需要输出，表示分类标签，908\n",
    "        y_test_point = np.array(y_test_point).astype(np.int32)\n",
    "    \n",
    "    return y_test_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot_new(model,verbose=0):\n",
    "\n",
    "    n_correct = 0\n",
    "    k = 908\n",
    "    preds = []\n",
    "    probs_all = []\n",
    "    err_print_num = 0\n",
    "    for idx in range(k):\n",
    "        inputs,targets,categories,y_test = test_data(idx) #通过函数调用测试数据\n",
    "        model.eval()\n",
    "        output1,output2 = model(inputs[0],inputs[1])  #需要改\n",
    "#         probs.detach() #将图中的变量解除出来\n",
    "        #probs = F.pairwise_distance(output1, output2)\n",
    "           \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "#         diff = euclidean_distance.cpu().detach().numpy()[0]\n",
    "#         print(euclidean_distance.cpu().detach().numpy()[0])\n",
    "\n",
    "\n",
    "        if np.argmin(euclidean_distance.detach().cpu().numpy()) == np.argmax(targets):\n",
    "#         if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "            \n",
    "        preds.append([categories[np.argmax(targets)],categories[np.argmin(euclidean_distance.detach().cpu().numpy())]])\n",
    "        probs_all.append(euclidean_distance.detach().cpu().numpy())\n",
    "\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    print(\"*\"*50)\n",
    "    print(n_correct)\n",
    "#     print(probs)\n",
    "\n",
    "    return percent_correct,np.array(preds),np.array(probs_all),categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "771\n",
      "84.91189427312776 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "758\n",
      "83.48017621145374 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "767\n",
      "84.47136563876651 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "760\n",
      "83.70044052863436 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "761\n",
      "83.81057268722466 (908, 2) (908, 10)\n",
      "5_shot: 86.78414096916299\n",
      "5_shot_prod: 84.91189427312776\n",
      "**************************************************\n",
      "753\n",
      "82.9295154185022 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "756\n",
      "83.25991189427313 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "748\n",
      "82.37885462555066 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "749\n",
      "82.48898678414096 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "738\n",
      "81.27753303964758 (908, 2) (908, 10)\n",
      "5_shot: 85.02202643171806\n",
      "5_shot_prod: 83.14977973568281\n",
      "**************************************************\n",
      "775\n",
      "85.35242290748899 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "769\n",
      "84.69162995594714 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "754\n",
      "83.03964757709251 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "773\n",
      "85.13215859030836 (908, 2) (908, 10)\n",
      "**************************************************\n",
      "769\n",
      "84.69162995594714 (908, 2) (908, 10)\n",
      "5_shot: 87.00440528634361\n",
      "5_shot_prod: 84.91189427312776\n"
     ]
    }
   ],
   "source": [
    "exps = [150,180,210]\n",
    "y_test = y_test_point()\n",
    "exp_name = \"test\"\n",
    "for exp in exps:\n",
    "    checkpoint = torch.load('./checkpoints_new/best_model_%s.pth'% exp)\n",
    "    model = get_model().cuda()\n",
    "#     model = nn.DataParallel(model,device_ids=[0,1,2])\n",
    "#     model = nn.DataParallel(model).cuda()\n",
    "#     device=torch.device(\"cuda:0\" )\n",
    "#     model.to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], False)\n",
    "\n",
    "    scores_1_shot = []\n",
    "    scores_5_shot = []\n",
    "    scores_5_shot_prod = []\n",
    "    \n",
    "    preds_5_shot = []\n",
    "    prods_5_shot = []\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for k in range(5):\n",
    "            val_acc,preds, prods, categories = test_oneshot_new(model)\n",
    "        #                 utils.confusion_plot(preds[:,1],preds[:,0])\n",
    "            print(val_acc,preds.shape, prods.shape)\n",
    "            scores.append(val_acc)\n",
    "        #                     print(\"preds[:,1]\",preds[:,1])\n",
    "            preds_5_shot.append(preds[:,1])\n",
    "            prods_5_shot.append(prods)\n",
    "        preds = []\n",
    "        #                 print(\"np.array(preds_5_shot).T:\", np.array(preds_5_shot).T)\n",
    "        for line in np.array(preds_5_shot).T:\n",
    "            pass\n",
    "            preds.append(np.argmax(np.bincount(line)))\n",
    "        #             utils.confusion_plot(np.array(preds),data.y_test) \n",
    "        prod_preds = np.argmin(np.sum(prods_5_shot,axis=0),axis=1).reshape(-1)\n",
    "\n",
    "        score_5_shot = accuracy_score(y_test,np.array(preds))*100\n",
    "        print('5_shot:',score_5_shot)\n",
    "\n",
    "        score_5_shot_prod = accuracy_score(y_test,prod_preds)*100\n",
    "        print('5_shot_prod:',score_5_shot_prod)\n",
    "\n",
    "        scores_1_shot.append(scores[0])\n",
    "        scores_5_shot.append(score_5_shot)\n",
    "        scores_5_shot_prod.append(score_5_shot_prod)\n",
    "\n",
    "        a =pd.DataFrame(np.array(scores_1_shot).reshape(-1))\n",
    "        a.to_csv(\"scores_1_shot.csv\")\n",
    "\n",
    "        a =pd.DataFrame(np.array(scores_5_shot).reshape(-1))\n",
    "        a.to_csv(\"scores_5_shot.csv\")\n",
    "\n",
    "        a =pd.DataFrame(np.array(scores_5_shot_prod).reshape(-1))\n",
    "        a.to_csv(\"scores_5_shot_prod.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# exps = [60,90,120,200,300,600,900]\n",
    "# for exp in exps:\n",
    "#     checkpoint = torch.load('./checkpoints/best_model_%s.pth'% exp)\n",
    "#     print(checkpoint['instance_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
